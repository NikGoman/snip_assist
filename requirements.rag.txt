# requirements.rag.txt
# Зависимости для сервиса RAG (rag/)

# FastAPI и ASGI сервер (если rag сам запускает FastAPI приложение)
fastapi>=0.115.0
uvicorn[standard]>=0.32.0

# LlamaIndex и его компоненты
llamaindex>=0.12.4
llamaindex-vector-stores-chroma>=0.4.2
# Выбор LLM зависит от реализации. Мы использовали TransformersLLM.
llamaindex-llms-transformers>=0.3.0
# Если бы использовался llama-cpp-python, было бы:
# llamaindex-llms-llama-cpp>=0.3.0 # (или другая версия)

# HuggingFace и PyTorch для LLM и эмбеддингов (если используется TransformersLLM и HF эмбеддинги)
transformers>=4.35.0
torch>=2.1.0 # Убедитесь, что версия соответствует вашему окружению (CPU/GPU)
# sentence-transformers>=2.2.0 # Может быть установлен как зависимость llama-index, но можно указать явно

# Pydantic для валидации данных
pydantic>=2.5.0
pydantic-settings>=2.1.0

# python-dotenv для загрузки переменных окружения из .env файлов
python-dotenv>=1.0.0

# (Опционально) Дополнительные библиотеки, которые могут потребоваться LlamaIndex или Chroma
# numpy>=1.21.0 # Обычно устанавливается как зависимость
# pandas>=1.3.0 # Обычно устанавливается как зависимость

# Убедитесь, что зависимости LlamaIndex (например, chromadb) также установлены.
# Они могут быть включены как транзитивные зависимости, но явное указание может повысить стабильность.
chromadb>=0.4.0
